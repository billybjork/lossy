# Voice-First Video Companion - Project Overview

**Last Updated:** 2025-10-14
**Status:** Pre-Implementation

---

## üéØ Project Goal

Build a **voice-first browser extension** that captures natural speech while reviewing videos and automatically generates structured, time-coded feedback that gets applied to the video platform (Air, YouTube, Vimeo, etc.) via intelligent automation.

### The Problem

Video editors and reviewers waste time:
- Typing detailed feedback with precise timestamps
- Context-switching between video player and comment interface
- Losing flow state to write clear, actionable notes
- Managing feedback across multiple video versions

### The Solution

Speak naturally while watching. The system:
1. **Captures** your voice with sub-second visual feedback (emoji chips)
2. **Transcribes** speech to text locally (WASM-first, private by default)
3. **Structures** raw transcripts into clear, actionable notes (LLM)
4. **Anchors** feedback to exact video timestamps and frames
5. **Posts** automatically to the video platform (Browserbase automation)

---

## üèóÔ∏è Technology Stack

### Frontend: Browser Extension (MV3)

| Component | Technology | Why |
|-----------|-----------|-----|
| **Extension Framework** | Chrome MV3 | Side Panel API, modern security |
| **UI - Popup** | Phoenix LiveView | Real-time agent progress streaming |
| **UI - Side Panel** | Phoenix LiveView | Persistent note list with live updates |
| **UI - Overlays** | Shadow DOM + Vanilla JS | On-video ghost comments, emoji chips |
| **Voice Capture** | MediaRecorder + VAD | @ricky0123/vad-web for speech detection |
| **Local STT** (Phase 6) | Transformers.js (Whisper) | WebGPU ‚Üí WASM ‚Üí Cloud fallback |
| **Local Vision** (Phase 7) | Transformers.js (SigLIP) | WebGPU ‚Üí WASM ‚Üí Skip fallback |
| **Bundler** | Webpack 5 | Local bundling of phoenix.js |

**Technology Fallback Hierarchy:**

*Transcription (STT):*
1. **Primary (MVP)**: OpenAI Whisper API (cloud)
2. **Phase 6**: Local WASM Whisper with WebGPU acceleration
3. **Fallback**: Auto-detect available RAM (<4GB ‚Üí cloud, ‚â•4GB ‚Üí local)
4. **User preference**: Settings toggle for cloud vs local

*Frame Analysis (CLIP/SigLIP for emoji chips):*
1. **Best**: WebGPU-accelerated SigLIP (50-150ms, Phase 7)
2. **Good**: WASM SigLIP (300-600ms, Phase 7)
3. **Fallback**: Skip emoji chips (not critical for MVP)
4. **Decision**: Check `navigator.gpu` support on init

*Memory Considerations:*
- Local Whisper: ~300MB model + ~200MB runtime
- Local SigLIP: ~100MB model
- Auto-fallback to cloud if available memory <4GB

### Backend: Phoenix/Elixir

| Component | Technology | Why |
|-----------|-----------|-----|
| **Web Framework** | Phoenix 1.7 | LiveView, Channels, PubSub |
| **Real-time** | Phoenix Channels | Binary WebSocket for audio streaming |
| **UI Framework** | Phoenix LiveView | Streaming timelines, reactive updates |
| **Agent State** | GenServer + PubSub | Supervised, observable sessions |
| **Database** | PostgreSQL | Structured storage, vector embeddings |
| **Background Jobs** | Oban | Note posting queue |
| **STT (cloud)** | OpenAI Whisper API | Cloud fallback/acceleration |
| **LLM** | OpenAI GPT-4o-mini | Note structuring, intent extraction |
| **Optional Local** | Rustler NIFs | whisper.cpp/llama.cpp acceleration |

### Automation: Browserbase

| Component | Technology | Why |
|-----------|-----------|-----|
| **Computer Use** | Browserbase API | Persistent browser sessions |
| **Automation** | Playwright + Stagehand | Traditional selectors + AI navigation |
| **Auth Management** | Browserbase Contexts | Persistent login state |
| **Language** | Python (existing) ‚Üí Elixir | Port existing agents, gradual migration |

---

## üé® Key Features

### MVP (Milestone 1)

1. **Voice Capture & Transcription**
   - Push-to-talk in popup/side panel
   - Local Whisper transcription (WASM)
   - Real-time transcript display

2. **Ghost Comments**
   - LLM structures raw speech into clear notes
   - Pinned to video timestamp
   - "Scratch that" to cancel
   - Confidence-based opacity

3. **Side Panel Note List**
   - LiveView streaming updates
   - Filter by video/category/status
   - Click to seek video timestamp

4. **Automated Posting**
   - Queue high-confidence notes
   - Browserbase automation
   - Status feedback in side panel

### Future Enhancements

- **Emoji Reasoning Tokens** (CLIP + late fusion)
- **Frame Analysis** (shot change detection, visual context)
- **Multi-note Merging** (consolidate nearby similar notes)
- **Voice Commands** ("scratch that", "post all", "undo")
- **Wake Word** (continuous listening mode)
- **Collaborative** (multi-user review sessions)

---

## üìä Performance Targets

Based on blueprint and research:

| Metric | Target | Notes |
|--------|--------|-------|
| **Listening Indicator** | ‚â§100ms | Video pause + anchor display |
| **Emoji Chips (WASM)** | 300-600ms | CLIP inference on frame |
| **Emoji Chips (WebGPU)** | 50-150ms | GPU-accelerated |
| **Ghost Comment** | 0.8-1.3s | Transcription + LLM structuring |
| **Note Posting** | 5-10s | Browserbase automation |
| **Frame Capture** | 20-60ms | Grab ‚Üí scale ‚Üí WebP encode |

---

## üé≠ User Experience Flow

```
1. User clicks mic (popup or side panel)
   ‚Üì
2. Video pauses, anchor chip shows timestamp
   ‚Üì
3. User speaks: "The pacing here is too slow"
   ‚Üì
4. [300ms] Emoji chip appears: üêå (pacing)
   ‚Üì
5. [1.2s] Ghost comment appears: "Slow pacing - speed up"
   ‚Üì
6. User can:
   - Say "scratch that" ‚Üí cancels
   - Do nothing ‚Üí auto-firms after 3s
   - Click "post" ‚Üí queues for automation
   ‚Üì
7. Background: Browserbase posts to video platform
   ‚Üì
8. Side panel updates: "Posted ‚úÖ"
```

---

## üîí Privacy & Data Flow

### Default: Local-First

```
Browser (WASM)                Backend (Cloud)
‚îú‚îÄ‚îÄ Audio capture            ‚îú‚îÄ‚îÄ Receives: transcript text only
‚îú‚îÄ‚îÄ STT (Whisper)           ‚îú‚îÄ‚îÄ Structures with LLM
‚îú‚îÄ‚îÄ Frame capture (CLIP)    ‚îú‚îÄ‚îÄ Stores: notes, timestamps
‚îî‚îÄ‚îÄ Emoji inference         ‚îî‚îÄ‚îÄ Automation: Browserbase

‚ùå NO audio sent to cloud by default
‚úÖ Only text + metadata + embeddings
```

### Optional: Cloud Acceleration

User can opt-in to send audio for:
- Higher quality STT (OpenAI Whisper API)
- Faster processing
- Advanced features (speaker diarization)

---

## üìÅ Repository Structure

```
lossy/
‚îú‚îÄ‚îÄ docs/                          # üìö Documentation (this directory)
‚îÇ   ‚îú‚îÄ‚îÄ 01_OVERVIEW.md            # This file
‚îÇ   ‚îú‚îÄ‚îÄ 02_PRINCIPLES.md          # Development principles
‚îÇ   ‚îú‚îÄ‚îÄ 03_ARCHITECTURE.md        # System design
‚îÇ   ‚îú‚îÄ‚îÄ sprints/                  # Sprint-based roadmap
‚îÇ   ‚îú‚îÄ‚îÄ 04_LIVEVIEW_PATTERNS.md
‚îÇ   ‚îî‚îÄ‚îÄ 05_BROWSERBASE_INTEGRATION.md
‚îÇ
‚îú‚îÄ‚îÄ lossy/                         # üî• Elixir/Phoenix application (@lossy namespace)
‚îÇ   ‚îú‚îÄ‚îÄ lib/lossy/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ accounts/             # User management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ videos/               # Video & note storage
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent/                # AgentSession GenServers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inference/            # STT/LLM routing
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ automation/           # Browserbase integration
‚îÇ   ‚îú‚îÄ‚îÄ lib/lossy_web/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ channels/             # Phoenix Channels
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ live/                 # LiveView modules
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ controllers/          # REST API
‚îÇ   ‚îî‚îÄ‚îÄ priv/python/              # Existing Python agents
‚îÇ
‚îî‚îÄ‚îÄ extension/                     # üß© Browser extension (MV3)
    ‚îú‚îÄ‚îÄ src/
    ‚îÇ   ‚îú‚îÄ‚îÄ background/           # Service worker
    ‚îÇ   ‚îú‚îÄ‚îÄ content/              # Content scripts + overlays
    ‚îÇ   ‚îú‚îÄ‚îÄ sidepanel/            # Side panel (LiveView client)
    ‚îÇ   ‚îú‚îÄ‚îÄ popup/                # Popup (LiveView client)
    ‚îÇ   ‚îî‚îÄ‚îÄ shared/               # Phoenix client, utilities
    ‚îú‚îÄ‚îÄ public/
    ‚îÇ   ‚îî‚îÄ‚îÄ models/               # ONNX models (cached)
    ‚îî‚îÄ‚îÄ manifest.json
```

---

## üöÄ Success Metrics

### Technical

- ‚úÖ Sub-second feedback (emoji chips)
- ‚úÖ < 1.5s ghost comments
- ‚úÖ 95%+ transcription accuracy (clear speech)
- ‚úÖ 90%+ note posting success rate
- ‚úÖ Zero manual timestamp entry

### User Experience

- ‚úÖ Maintains flow state (no keyboard context switch)
- ‚úÖ Clear visual feedback at every step
- ‚úÖ Graceful degradation (works offline, slow networks)
- ‚úÖ Undo/cancel at any point

### Business

- ‚úÖ 10x faster feedback generation vs manual typing
- ‚úÖ 100% time-coded notes (vs ~20% manual)
- ‚úÖ Platform-agnostic (works on Air, YouTube, Vimeo, etc.)

---

## üéì Key Learnings Applied

From research and prototype:

1. **WASM-first, not cloud-first** - Privacy + speed
2. **LiveView for extension UI** - Real-time streaming perfect for agent progress
3. **Chained architecture** - OpenAI guidance, easier than realtime voice
4. **Browserbase Contexts** - Persistent auth, no credential storage
5. **Stagehand > selectors** - AI navigation more robust than brittle CSS
6. **Phoenix Channels for binary** - Efficient audio streaming
7. **Side Panel > Popup** - Persistent UI for note list

---

## üìö References

- **Research:** Conducted 2025-10-14 (WASM inference, LiveView patterns, etc.)
- **Archived docs:** See `docs/archive/` for blueprint and earlier implementation plans

---

## ‚ö° Quick Start (After Implementation)

```bash
# Backend
cd lossy
mix deps.get
mix ecto.setup
mix phx.server

# Extension
cd extension
npm install
npm run build
# Load unpacked extension from extension/dist/
```

See `sprints/` for sprint-by-sprint implementation plan.
